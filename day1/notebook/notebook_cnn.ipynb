{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we create a CNN network for the MNIST dataset with the following architecture:\n",
    "* **Conv. layer 1:** computes 32 feature maps using a 5x5 filter with ReLU activation.\n",
    "* **Pooling layer 1:** max pooling layer with a 2x2 filter and stride of 2.\n",
    "* **Conv. layer 2:** computes 64 feature maps using a 5x5 filter.\n",
    "* **Pooling layer 2:** max pooling layer with a 2x2 filter and stride of 2.\n",
    "* **Dense layer:** densely connected layer with 1024 neurons.\n",
    "* **Logits layer**\n",
    "\n",
    "First we make the network manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual building layers\n",
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 100\n",
    "\n",
    "########################################\n",
    "# load the mnist data\n",
    "########################################\n",
    "mnist = mnist_data.read_data_sets(\"data\", one_hot=True, reshape=False, validation_size=0)\n",
    "\n",
    "########################################\n",
    "# define placeholders\n",
    "########################################\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y_true = tf.placeholder(tf.float32, [None, 10])\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "########################################\n",
    "# build the model\n",
    "########################################\n",
    "# Convolutional Layer #1\n",
    "# Computes 32 feature maps using a 5x5 filter with ReLU activation.\n",
    "# Padding is added to preserve width and height.\n",
    "# Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "# Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "W1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))  # 5x5 filter, 1 input channel, 32 output channels\n",
    "B1 = tf.Variable(tf.constant(0.1, tf.float32, [32]))\n",
    "stride = 1  # output is 28x28\n",
    "conv1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding=\"SAME\") + B1)\n",
    "\n",
    "# Pooling Layer #1\n",
    "# First max pooling layer with a 2x2 filter and stride of 2\n",
    "# Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "# Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "# Convolutional Layer #2\n",
    "# Computes 64 features using a 5x5 filter.\n",
    "# Padding is added to preserve width and height.\n",
    "# Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "# Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "W2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1))  # 5x5 filter, 32 input channel, 64 output channels\n",
    "B2 = tf.Variable(tf.constant(0.1, tf.float32, [64]))\n",
    "stride = 1  # output is 28x28\n",
    "conv2 = tf.nn.relu(tf.nn.conv2d(pool1, W2, strides=[1, stride, stride, 1], padding=\"SAME\") + B2)\n",
    "\n",
    "# Pooling Layer #2\n",
    "# Second max pooling layer with a 2x2 filter and stride of 2\n",
    "# Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "# Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "# Flatten tensor into a batch of vectors\n",
    "# Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "# Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "# Dense Layer\n",
    "# Densely connected layer with 1024 neurons\n",
    "# Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "# Output Tensor Shape: [batch_size, 1024]\n",
    "W3 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1024], stddev=0.1))  # 7x7x64 feature, 1024 neurons\n",
    "B3 = tf.Variable(tf.constant(0.1, tf.float32, [1024]))\n",
    "dense = tf.nn.relu(tf.matmul(pool2_flat, W3) + B3)\n",
    "\n",
    "# Add dropout operation\n",
    "dropout = tf.nn.dropout(dense, pkeep)\n",
    "\n",
    "# Logits layer\n",
    "# Input Tensor Shape: [batch_size, 1024]\n",
    "# Output Tensor Shape: [batch_size, 10]\n",
    "W4 = tf.Variable(tf.truncated_normal([1024, 10], stddev=0.1))  # 1024 feature, 10 neurons\n",
    "B4 = tf.Variable(tf.constant(0.1, tf.float32, [10]))\n",
    "logits = tf.matmul(dropout, W4) + B4\n",
    "y_hat = tf.nn.softmax(logits)\n",
    "\n",
    "########################################\n",
    "# define the cost and accuracy functions\n",
    "########################################\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_true)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy) * 100\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_hat, 1), tf.argmax(y_true, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "########################################\n",
    "# define the optimizer\n",
    "########################################\n",
    "lr = 0.003\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "########################################\n",
    "# execute the model\n",
    "########################################\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        # load batch of images and correct answers\n",
    "        batch_X, batch_y = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # learning rate decay\n",
    "        if i % 100 == 0:\n",
    "            a, c = sess.run([accuracy, cross_entropy], feed_dict={X: batch_X, y_true: batch_y, pkeep: 1.0})\n",
    "            print('epoch {}: accurecy = {}, loss = {}'.format(i, a, c))\n",
    "\n",
    "        # train\n",
    "        sess.run(train_step, feed_dict={X: batch_X, y_true: batch_y, pkeep: 0.75})\n",
    "        \n",
    "    a, c = sess.run([accuracy, cross_entropy], feed_dict={X: mnist.test.images, y_true: mnist.test.labels, pkeep: 1.0})\n",
    "    print('test data: accurecy = {}, loss = {}'.format(a, c))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we take advantage of the `tf.layers` to build the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 100\n",
    "\n",
    "########################################\n",
    "# load the mnist data\n",
    "########################################\n",
    "mnist = mnist_data.read_data_sets(\"data\", one_hot=True, reshape=False, validation_size=0)\n",
    "\n",
    "#######################################\n",
    "# defineplaceholders\n",
    "########################################\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y_true = tf.placeholder(tf.float32, [None, 10])\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "########################################\n",
    "# build the model\n",
    "########################################\n",
    "# Convolutional Layer #1\n",
    "# Computes 32 feature maps using a 5x5 filter with ReLU activation.\n",
    "# Padding is added to preserve width and height.\n",
    "# Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "# Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "conv1 = tf.layers.conv2d(inputs=X, filters=32, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\n",
    "\n",
    "# Pooling Layer #1\n",
    "# First max pooling layer with a 2x2 filter and stride of 2\n",
    "# Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "# Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "# Convolutional Layer #2\n",
    "# Computes 64 features using a 5x5 filter.\n",
    "# Padding is added to preserve width and height.\n",
    "# Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "# Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\n",
    "\n",
    "# Pooling Layer #2\n",
    "# Second max pooling layer with a 2x2 filter and stride of 2\n",
    "# Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "# Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "# Flatten tensor into a batch of vectors\n",
    "# Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "# Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "# Dense Layer\n",
    "# Densely connected layer with 1024 neurons\n",
    "# Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "# Output Tensor Shape: [batch_size, 1024]\n",
    "dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "# Add dropout operation\n",
    "dropout = tf.layers.dropout(inputs=dense, rate=pkeep)\n",
    "\n",
    "# Logits layer\n",
    "# Input Tensor Shape: [batch_size, 1024]\n",
    "# Output Tensor Shape: [batch_size, 10]\n",
    "logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "y_hat = tf.nn.softmax(logits)\n",
    "\n",
    "########################################\n",
    "# define the cost and accuracy functions\n",
    "########################################\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_true)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy) * 100\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_hat, 1), tf.argmax(y_true, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "########################################\n",
    "# define the optimizer\n",
    "########################################\n",
    "lr = 0.003\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "########################################\n",
    "# execute the model\n",
    "########################################\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        # load batch of images and correct answers\n",
    "        batch_X, batch_y = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # learning rate decay\n",
    "        if i % 100 == 0:\n",
    "            a, c = sess.run([accuracy, cross_entropy], feed_dict={X: batch_X, y_true: batch_y, pkeep: 1.0})\n",
    "            print('epoch {}: accurecy = {}, loss = {}'.format(i, a, c))\n",
    "\n",
    "        # train\n",
    "        sess.run(train_step, feed_dict={X: batch_X, y_true: batch_y, pkeep: 0.75})\n",
    "        \n",
    "    a, c = sess.run([accuracy, cross_entropy], feed_dict={X: mnist.test.images, y_true: mnist.test.labels, pkeep: 1.0})\n",
    "    print('test data: accurecy = {}, loss = {}'.format(a, c))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use Keras to build the above network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 100\n",
    "\n",
    "########################################\n",
    "# load the mnist data\n",
    "########################################\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Rscaling of the pixel values from 0 to 255 to the range between 0 and 1. It improves the learning speed.\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "########################################\n",
    "# buid the model\n",
    "########################################\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=[5, 5], activation='relu', input_shape=[28, 28, 1]))\n",
    "model.add(MaxPooling2D(pool_size=[2, 2], strides=2))\n",
    "model.add(Conv2D(64, kernel_size=[5, 5], activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=[2, 2], strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
